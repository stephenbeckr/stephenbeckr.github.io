---
layout: page_with_toc
title: Papers
permalink: /papers/
---

For the very latest, see autogenerated lists, like:
- [Stephen's arXiv papers](http://arxiv.org/a/becker_s_1)
- [google scholar](http://scholar.google.com/citations?hl=en&user=fUDbcnYAAAAJ)
- [DBLP](https://dblp.org/pid/47/9100) computer science papers
- [ResearchGate](https://www.researchgate.net/profile/Stephen_Becker5) and [academia.edu](https://colorado.academia.edu/StephenBecker)
- [ORCID](http://orcid.org/0000-0002-1932-8159)
- [Semantic Scholar](https://www.semanticscholar.org/author/50333204)

<!-- see https://github.com/jwrr/minima-sidebar/blob/master/sidebar-fixed.markdown -->
<!--
<nav class="toc-fixed" markdown="1">
* TOC
{:toc}
</nav>
-->

Click the title or triangle to show *abstracts*

<!-- See https://jekyllrb.com/docs/datafiles/ 
and https://github.com/Shopify/liquid/wiki/Liquid-for-Designers 

There's a secret special value empty (unquoted) that you can compare arrays to; the comparison is true if the array has no publications.

Liquid expressions are tested for "truthiness" in what looks like a Ruby-like way:

true is true.
false is false.
Any string is true, including the empty string.
Any array is true.
Any hash is true.
Any nonexistent/nil value (like a missing publication of a hash) is false.

In Liquid, you can include a hyphen in your tag syntax using minus sign, see https://stackoverflow.com/questions/35642820/jekyll-how-to-use-for-loop-to-generate-table-row-within-the-same-table-inside-m

-->
## Preprints {#preprints}
<ul class="papers" id="preprints">
{% for publication in site.data.papers %}
{% if publication.unpublished == true %}
  <div class="selection"> <details class="papers" id="{{ publication.id }}"><summary>{{ publication.year }} <span class="title">{{ publication.title }}</span></summary><b>Abstract</b>: {{ publication.abstract }}</details>
  <ul> <li>
<span class="author">  {{ publication.author }}.</span> 
<span class="venue">{{ publication.venue }}</span><span class="pubMisc">{{ publication.misc }}</span>
{% if publication.DOI != null %}
[<a href="{{ publication.DOI }}">DOI</a>]
    {% endif %}

    {% if publication.arXiv != null %}
    [<a href="https://arxiv.org/abs/{{ publication.arXiv | string }}">arXiv</a>]
    {% endif %}

    {% if publication.code != null %}
    [<a href="{{ publication.code }}">code</a>]
    {% endif %}

    {% if publication.other-link != null %}
    {{ publication.other-link }}
    {% endif %}

    </li></ul></div>
{% endif %}
{% endfor %}
</ul>

## Journal papers {#journals}
<ul class="papers" id="journals">

<!-- First, find out total # of papers so I can count backwards -->
{% assign number_journal_papers = 0 %}
{% assign number_conference_papers = 0 %}
{% for publication in site.data.papers %}
{% if publication.unpublished == null or publication.unpublished == false %}
{% if publication.conference == null or publication.conference == false %}
{% assign number_journal_papers = number_journal_papers | plus: 1 %}
{% else %}
{% assign number_conference_papers = number_conference_papers | plus: 1 %}
{% endif %}
{% endif %}
{% endfor %}

{% assign current_paper = number_journal_papers %}
{% for publication in site.data.papers %}
{% if publication.conference == null or publication.conference == false %}
{% if publication.unpublished == null or publication.unpublished == false %}
   <div class="selection"><details class="papers" id="{{ publication.id }}"><summary>[J{{current_paper}}] {{ publication.year }} <span class="title">{{ publication.title }}</span></summary><b>Abstract</b>: {{ publication.abstract }}</details>
  <ul> <li>
<span class="author">  {{ publication.author }}.</span> 
<span class="venue">{{- publication.venue -}}</span><span class="pubMisc">{% if publication.misc != null %}, {{ publication.misc -}}.
<!-- <em>{{ publication.venue -}}</em> -->
<!-- test if fields exist, so that I don't get extra commas -->
{%- else -%}.
{% endif %}</span>
{% assign current_paper = current_paper | minus: 1 %}
    {% if publication.DOI != null %}
    [<a href="{{ publication.DOI }}">DOI</a>]
    {% endif %}

    {% if publication.arXiv != null %}
    [<a href="https://arxiv.org/abs/{{ publication.arXiv }}">arXiv</a>]
    {% endif %}

    {% if publication.code != null %}
    [<a href="{{ publication.code }}">code</a>]
    {% endif %}

    {% if publication.other-link != null %}
    {{ publication.other-link }}
    {% endif %}
    </li></ul></div>
{% endif %}
{% endif %}
{% endfor %}
</ul>

## Conference papers {#conference}
<ul class="papers" id="conference">
{% assign current_paper = number_conference_papers %}
{% for publication in site.data.papers %}
{% if publication.conference == true %}
{% if publication.unpublished == null or publication.unpublished == false %}
   <div class="selection"><details class="papers" id="{{ publication.id }}"><summary>[C{{current_paper}}] {{ publication.year }} <span class="title">{{ publication.title }}</span></summary><b>Abstract</b>: {{ publication.abstract }}</details>
  <ul> <li >
<span class="author">  {{ publication.author }}.</span> 
<span class="venue">{{- publication.venue -}}</span><span class="pubMisc">{% if publication.misc != null %}, {{ publication.misc -}}.{%- else -%}.
{% endif %}</span>
        {% assign current_paper = current_paper | minus: 1 %}

    {% if publication.DOI != null %}
    [<a href="{{ publication.DOI }}">DOI</a>]
    {% endif %}

    {% if publication.arXiv != null %}
    [<a href="https://arxiv.org/abs/{{ publication.arXiv }}">arXiv</a>]
    {% endif %}

    {% if publication.code != null %}
    [<a href="{{ publication.code }}">code</a>]
    {% endif %}

    {% if publication.other-link != null %}
    {{ publication.other-link }}
    {% endif %}
    </li></ul></div>
{% endif %}
{% endif %}
{% endfor %}
</ul>

## Book chapters {#chapters}
- Dual Smoothing Techniques for Variational Matrix Decomposition, S. Becker and A. Aravkin, in “Robust Low-Rank and Sparse Matrix Decomposition: Applications in Image and Video Processing”, T. Bouwmans, N. Aybat, E. Zahzah, eds. CRC Press, 2016. [arXiv](http://arxiv.org/abs/1603.00284)

## Selected Talks {#talks}

Here I list a few talks that have video or slides or websites

- "Research in Applied Math at CU", Math Circles talk (K-12), Sept 15 2024.  [Slides](/assets/docs/MathCircles_Sept15_2024.pdf).
- "Randomization methods for big-data", at CU CS Dept colloquium Oct 26 2023, and Purdue Math dept Applied Math seminar Oct 23 2023.  [Slides](/assets/docs/2023_10_CU_CS_RandomMethods.pdf).
  - For related sketching code:
  - [Random sketching operators](https://github.com/stephenbeckr/randomized-algorithm-class/tree/master/Code) in Matlab and Python, as part of the class I teach. This has Gaussian sketch, Fast Johnson Lindenstrauss (with FFT or a fast Hadamard), and count sketch. The Hadamard and count sketch have C functions to make them extra fast.  
  - For the tensor product sketches, which is more complicated, see [Osman's github page](https://github.com/OsmanMalik/)
- "High-Probability convergence and algorithmic stability for stochastic gradient descent", [ML-MTP: Machine Learning in Montpellier: Theory and Practice](https://groupes.renater.fr/wiki/ml-mtp/index) seminar, September 2022. [Slides](/assets/docs/2022_Sept_IMAG.pdf).  Similar versions given in Arcachon (Curves and Surfaces, June 2022) and Nice (October 2022)
- "Stochastic Subspace Descent: Stochastic gradient-free optimization, with applications to PDE-constrained optimization"   [Slides (2.7 MB pdf)](/assets/docs/2022_ICCOPT_Lehigh_SSD_Becker.pdf), at ICCOPT, Lehigh University July 2022.  Similar version given January 2020 in Denver (joint MAA meetings).
- "Optimization for statistical estimators: Applications to quantum fidelity estimation" [Slides (3.7 MB pdf)](/assets/docs/2022_Stockholm_Becker_midRes.pdf), at Conference on the Mathematics of Complex Data, KTH Royal Institute of Technology, Stockholm. June 13-16 2022
- "Matrix Completion and Robust PCA", University of Colorado Boulder, Computer Science department colloquium, Boulder, CO. Nov 20 2014. [video](http://vimeo.com/channels/cucs)
- [Advances in first-order methods: constraints, non-smoothness and faster convergence](
https://fadili.users.greyc.fr/siamis12/minisymposium2.html), Minisymposium, SIAM Imaging Science, Philadelphia. [Slides](https://fadili.users.greyc.fr/siamis12/minisymposium2/BeckerIS12.pdf), May 22 2012
- ["TFOCS: Flexible First-order Methods for Rank Minimization"](/assets/docs/SIOPT2011_Becker.pdf), at the [Low-rank Matrix Optimization minisymposium](http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=12168) at the 2011 SIAM Conference on Optimization in Darmstadt. Here's the [RPCA video](http://www.youtube.com/watch?v=Yxj1_52EAXA) (Matlab code to generate this can be found at the [Demo page on the TFOCS website](http://cvxr.com/tfocs/demos/rpca/).  May 19 2011
- [Quick intro to convex optimization](/assets/docs/convexOptTalk.pdf) talk for Patrick Sanan's department "ACM^tea", Oct 23 2009

## Technical Reports {#techReports}
- [Variational Entropy Search for Adjusting Expected Improvement](https://arxiv.org/abs/2402.11345), Nuojin Cheng, Stephen Becker, 2024. Superseded by [A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization](https://arxiv.org/abs/2501.18756), Nuojin Cheng, Leonard Papenmeier, Stephen Becker, Luigi Nardi, 2025.
- [ Locality-sensitive hashing in function spaces](https://arxiv.org/abs/2002.03909), Will Shand and Stephen Becker, 2020
- [Tensor Robust Principal Component Analysis: Better recovery with atomic norm regularization](https://arxiv.org/abs/1901.10991), Derek Driggs, Stephen Becker and Jordan Boyd-Graber, 2019
- [URV Factorization with Random Orthogonal System Mixing](https://arxiv.org/abs/1703.02499), Stephen Becker, James Folberth, Laura Grigori, 2017
- [The Chen-Teboulle algorithm is the proximal point algorithm](https://arxiv.org/abs/1908.03633), from 2011 but posted 2019, shows the Chen-Teboulle algorithm admits a more aggressive stepsize than via the original analysis.
- [Exact linesearch for LASSO](/assets/docs/exactLinesearchL1.pdf) discusses exact step-size selection for piecewise quadratic objective functions, with [code](https://github.com/stephenbeckr/exactLASSOlinesearch). 2016.


## Theses {#theses}

- 2011 [Practical Compressed Sensing: modern data acquisition and signal processing](http://resolver.caltech.edu/CaltechTHESIS:06022011-152525054), California Institute of Technology (PhD thesis). Co-winner, Carey prize. <details> Since 2004, the field of compressed sensing has grown quickly and seen tremendous interest because it provides a theoretically sound and computationally tractable method to stably recover signals by sampling at the information rate. This thesis presents in detail the design of one of the world's first compressed sensing hardware devices, the random modulation pre-integrator (RMPI). The RMPI is an analog-to-digital converter (ADC) that bypasses a current limitation in ADC technology and achieves an unprecedented 8 effective number of bits over a bandwidth of 2.5 GHz. Subtle but important design considerations are discussed, and state-of-the-art reconstruction techniques are presented. Inspired by the need for a fast method to solve reconstruction problems for the RMPI, we develop two efficient large-scale optimization methods, NESTA and TFOCS, that are applicable to a wide range of other problems, such as image denoising and deblurring, MRI reconstruction, and matrix completion (including the famous Netflix problem). While many algorithms solve unconstrained l1 problems, NESTA and TFOCS can solve the constrained form of l1 minimization, and allow weighted norms. In addition to l1 minimization problems such as the LASSO, both NESTA and TFOCS solve total-variation minimization problem. TFOCS also solves the Dantzig selector and most variants of the nuclear norm minimization problem. A common theme in both NESTA and TFOCS is the use of smoothing techniques, which make the problem tractable, and the use of optimal first-order methods that have an accelerated convergence rate yet have the same cost per iteration as gradient descent. The conic dual methodology is introduced in TFOCS and proves to be extremely flexible, covering such generic problems as linear programming, quadratic programming, and semi-definite programming. A novel continuation scheme is presented, and it is shown that the Dantzig selector benefits from an exact-penalty property. Both NESTA and TFOCS are released as software packages available freely for academic use. </details>

- 2005 [Translational and Rotational Dynamics of Supercooled Water](/assets/docs/thesis.pdf), Wesleyan University (undergraduate thesis). Co-winner, Bertman prize. <details> This thesis presents the results of extensive simulations of the ST2 model of water designed to explore both thermodynamic and dynamic properties in super- cooled states. Constant volume simulations were performed at 1,233 state points, with densities ranging from 0.80 g/cm3 to 1.20 g/cm3 in steps of .01 g/cm3; along these isochores, simulations were run at temperatures from 250 K to 400 K in 5 K intervals. Our results qualitatively reproduce many of the expected properties of supercooled water such as the density anomaly and a novel low-temperature phase transition between two liquid states. The Stokes-Einstein and Debye-Stokes- Einstein equations provide a simple hydrodynamic relation between viscosity and diffusion. These relations hold for simple liquids, but are known to fail near a glass transition. For ST2 water, we find that the Debye-Stokes-Einstein equation does not hold at low temperatures. Furthermore, the data indicate that rotational diffusion is enhanced at low temperatures relative to translational diffusion. We also uncover an unexpected connection between dynamics and thermodynamics; specifically, we find that the structural relaxation time τα along the critical iso-chore provides a precursor to the liquid-liquid phase transition at temperatures up to 150 K above the critical temperature. We observe a nearly identical sig- nature in τα at a higher density which may indicate a second liquid-liquid phase transition, adding credibility to the recent idea that there may be more than one liquid-liquid critical point. </details>

### Student theses
Theses of my students (after 2019, CU stopped posting these online, so I'm posting local copies when I haven them):

In the pipeline
- K. Aditi (PhD), 202?
- Nic Rummel (PhD), 202?
- Drona Khurana, 202?

PhD theses
- [Multi-fidelity Uncertainty Quantification and Optimization](/assets/docs/NuojinCheng_PhDthesis_2025), Nuojin Cheng, 2025, PhD (main advisor: Alireza Doostan, Aerospace). <details> This thesis presents four novel bi-fidelity modeling approaches designed to enhance computational e!ciency and accuracy in uncertainty quantification and optimization. First, Bi-fidelity Boosting (BFB) introduces an e”ective sketching-based subsampling method, accompanied by theoretical analysis of how inter-model correlation impacts performance. Second, the Bi-fidelity Variational Auto-encoder (BF-VAE) leverages deep generative models and transfer learning to achieve high performance with minimal high-fidelity data, also revealing connections between multi-fidelity learning and information bottleneck theory. Third, Langevin Bi-fidelity Importance Sampling (L-BF-IS) develops an efficient score-based Metropolis-Hastings importance sampling estimator for uncertainty quantification, whose e”ectiveness is linked to the discrepancy between model failure probability measures. Finally, a bi-fidelity zero-order optimization framework employs local multi-fidelity surrogates and an Armijo-based line search for optimal step sizes, demonstrating strong empirical performance supported by theoretical convergence guarantees under specific conditions. Collectively, these contributions advance multi-fidelity modeling by providing efficient, theoretically grounded methods for tackling complex computational challenges. </details>
- [Computational Methods of Optimization and Geometry Processing for Physics Applications](/assets/docs/JacobSpainhour_PhDthesis_2025), Jacob Spainhour, 2025, PhD. <details> 
This thesis presents novel computational methods in two distinct, yet not disjoint subfields of applied mathematics, and is consequently divided into two parts. In Part I, we consider methods of numerical optimization as applied to problems in scientific experimentation, in which a limited quantity of available resources must be carefully allocated to maximize the performance of the experiment. In the first chapter of Part I, this takes the for of discovering optimized ultrasound transmission sequences which, when imaged in the REFoCUS framework, produce high quality images. In the second chapter of Part I, this takes the form of discovering optimized measurement protocols for the verification of quantum states through fidelity estimation.
In Part II, we consider methods of computational geometry as applied to problems in multiphysics simulation, in which geometrically complex objects must be computationally handled in a way that is reliable and robust to numerical imperfection. In the first and second chapter of Part II, we develop methods of evaluating the generalized winding number for general curves and surfaces, from which we derive methods of determining containment for geometric objects which otherwise do not define an interior volume. In the third chapter of Part II, we develop a method of 3D material interface reconstruction by means of optimally defining a collection of half-spaces whose convex intersection has geometric moments as close as possible to provided reference data. </details>
- [Minimax Optimal Estimation of Expectation Values](/assets/docs/NOTPOSTED), Akshay Seshadri, 2025, PhD (co-advised with Emanuel Knill, NIST) <details>  
- [Numerical Methods for Non-uniform Data Sources](https://www.proquest.com/docview/3100721110/FF9F64101B99442DPQ) Kevin Doherty, 2024, PhD. [local PDF copy](/assets/docs/KevinDoherty_thesis2024.pdf) <details> This thesis surveys and creates methods to allow for a mathematically consistent treatment of non-uniform data sources in machine learning and data compression. These methods are fundamentally rooted in numerical methods for quadrature and interpolation, but are leveraged with appropriate computational tools and techniques to adapt to their respective domains and problem types. </details>
- [Mathematical Formulations with Uncertain Data in
Optimization and Inverse Problems](https://www.rjclancy.com/assets/documents/dissertation.pdf) Richard Clancy, 2022, PhD <details> This dissertation broadly focuses on incorporating uncertainty into mathematical models and
can be broken into three distinct sections. In the first (Ch. 2 and 3), we consider design matrix uncertainty for linear regression. The motivation is that many regressors or covariates used to build linear models, such as survey responses and other data subjective in nature, are intrinsically uncertain. We approach the problem from two angles: 1) using robust optimization and 2) an approximation method to construct an otherwise cumbersome objective for maximum likelihood estimation. Expressions for (sub)gradients are provided allowing for the use of efficient solvers. We illustrate the merit of both methods on synthetic data. In the second section (Ch. 4 and 5), we focus on novel trust region methods that accept uncertainty in objective and gradient evaluations to reduce the computational burden. We introduce an algorithm named TROPHY (Trust Region Optimization using a Precision HierarchY) which uses variable precision arithmetic to reduce communication, storage, and energy costs. The other project uses a Hermite interpolation based framework to build model objectives using function and derivative information. Since not all partial derivatives are available or are expensive to compute, we make use of uncertain gradients to improve performance beyond standard interpolation methods. We also propose the use of sketching to ameliorate issues with redundant data and reduce the burden of inverting very large matrices. The final section (Ch. 6) investigates how operator uncertainty impacts the ability to solve inverse problems accurately. Our goal is to localize regions of brain activity using optically pumped magnetometers which are novel sensors that show promise for use in magnetoencephalography. Through a series of simulations, we establish guidelines for sensor count, noise level, and forward model fidelity needed to localize brain activity to a specified accuracy. </details>
- [First-Order Methods for Online and Stochastic Optimization, and Approximate Compiling](https://www.proquest.com/docview/2669410131/310C41AC99D04956PQ)
Liam Madden, 2022, PhD (co-advised with Emiliano Dall'Anese, CU ECEE). [local PDF copy](/assets/docs/LiamMadden_thesis2022.pdf) <details> The oracle complexity model for optimization has been the source of many advances in
optimization algorithms with applications to machine learning, artificial intelligence, and highdimensional statistics. For example, many adaptive methods have their beginnings as algorithms
with tune-able hyper-parameters appearing in theoretical complexity bounds. This thesis investigates three different problems from the oracle complexity model.
The first problem, presented in Chapter 2, considers the online smooth convex optimization
problem, motivated by time-varying applications. Two important results are a lower bound on the upper bounds of general online first-order methods and an algorithm with a matching upper bound. The algorithm presented applies Nesterov’s method to a stale cost function until sufficient convergence has been achieved at which point the cost function is updated to the current one.
The second problem, presented in Chapter 3, considers the approximate compiling problem of quantum computing. We focus on the CNOT+rotation gate set and consider three different CNOT patterns. Despite the non-convexity of the problem, we find that local minima perform well for random target circuits. We also find that simple CNOT patterns seem to achieve the lower bound on the number of CNOTs required to exactly compile random target circuits. We also use the optimization framework to explore and find new decompositions of particular target circuits such as the Toffoli gate.
The third problem, presented in Chapter 4, considers the stochastic smooth non-convex optimization problem, motivated by machine learning applications. Two important results are a Freedman-type concentration inequality that breaks through the sub-exponential threshold to heavier-tailed martingale difference sequences and a high probability convergence bound for stochastic gradient descent with sub-Weibull gradient noise. </details>
- [Measuring Image Resolution in Super-Resolution Microscopy and Bayesian Estimation of Population Size and Overlap and Vaccine Effectiveness](https://www.proquest.com/docview/2572564388/3313CC654D214068PQ) Erik Johnson, 2021, PhD (main advisor Dan Larremore, CU CS) <details> This thesis is a compilation of three separate projects. Although the three projects are quite distinct in that they do not directly build on one another, the common thread is statistics and computation applied to medicine and biology. In the first project, we develop a Bayesian statistical model to quantify the size of and overlap between two populations from subsamples from each population. Our model produces joint estimates of the size of each population and the amount of overlap between the populations. We apply our model to compare the genetic repertoires of malaria parasites. In particular, we use our method to construct parasite genetic similarity networks with which we analyze the evolution and transmission of malaria in Ecuador before and after an outbreak in 2012-2013. We also use our method’s inferences to argue that the polymerase chain reaction primers used to study an important malaria parasite gene are biased. In the second project, motivated by the COVID-19 pandemic, we develop a Bayesian statistical model to estimate vaccine effectiveness using data from a common vaccine effectiveness study design called a test-negative design. Since disease positivity is typically assessed using diagnostic tests that have imperfect sensitivity and specificity, our model adjusts for imperfect diagnostic tests along with other potential confounders. In addition, our framework allows data streams from multiple diagnostic test types to be rigorously combined which makes our model flexible for analyses and meta-analyses of test-negative designs. In the third project, we study image resolution metrics that are applicable to modern super-resolution fluorescent microscopy techniques. After showing that the primary existing quantitative image resolution metric called Fourier ring correlation is not ideal, we argue that image resolution in super-resolution microscopy might best be analyzed using information theory. Towards that end, we propose a mutual information-based image resolution metric and we study our metric’s behavior through a few simple examples. </details>
- [Topics in Matrix and Tensor Computations](https://www.proquest.com/docview/2572559828/2D96D8173E184A7CPQ/) Osman Malik, 2021, PhD. [local PDF copy](/assets/docs/OsmanMalik_thesis2021.pdf) <details> This dissertation looks at matrices and tensors from a computational perspective. An important problem in both matrix and tensor computations is decomposition. Several chapters in this dissertation deal with this problem. Chapter 2 develops randomized algorithms for Tucker tensor decomposition which are faster and can handle larger datasets than competing methods. These algorithms are the first ever one-pass methods for Tucker decomposition to appear in the literature. Chapter 4 develops randomized algorithms with guarantees for both matrix and tensor interpolative decomposition (ID). A key contribution is the first ever theoretical guarantees for any randomized tensor ID algorithm. Chapter 7 develops a randomized algorithm for the tensor ring decomposition. By using leverage score sampling, the resulting iterative algorithm has a per iteration cost which is sublinear in the number of input tensor entries. Chapter 8 considers the binary matrix factorization problem and develops a QUBO formulation for solving it on special purpose hardware like the Fujitsu Digital Annealer and the D-Wave Quantum Annealer. In the tensor decomposition works mention above, randomization—more specifically, matrix sketching—is a key tool used to make algorithms faster and more efficient. In Chapter 3 we dive deeper into this topic and consider one such sketch, the Kronecker fast Johnson–Lindenstrauss transform (KFJLT). We provide a novel proof that the KFJLT indeed is a Johnson–Lindenstrauss transform. Randomization is also used in Chapter 5 to improve the performance of fast matrix multiplication algorithms plagued by either numerical rounding error or error in the computation formula itself. An upshot of this work is a simple method that can help make fast algorithms such as Strassen’s more robust, especially on low-precision hardware like GPUs. Machine learning is a field where tensor methods have received considerable attention. In Chapter 6, we develop a tensor based graph neural network that can be used for a variety of prediction tasks on time varying graphs. We achieve competitive performance in prediction tasks, including in a COVID-19 contact tracing application. </details>
- [Randomization in Statistical Machine Learning](https://www.proquest.com/docview/2447014909/80A21D291D8B4D9BPQ) Zhishen (Leo) Huang, 2020, PhD. [local PDF copy](/assets/docs/ZhishenHuang_thesis2020.pdf) <details> Supervised learning and reinforcement learning problems are often formulated as optimization problems for training. The optimization algorithms themselves bear interest from the mathematical point of view. This thesis discusses the usage of randomization in optimization, which makes possible what corresponding deterministic algorithms are unable to achieve. Applying randomization in algorithms has the capability of reducing the time or space complexity at the expense of potential failure to provide a valid solution. Early prominent examples of randomized algorithms include quicksort, Karger’s algorithm for min-cut problem and the Bloom filter. In this thesis, randomization is introduced into optimization algorithms and is used for data compression. This thesis considers first-order optimization algorithms due to their practicality for large- scale application. The first chapter considers the minimization of nonconvex and nonsmooth objectives, where we give probabilistic guarantees for the proximal gradient descent method to converge to local minima [HB20a]. The second chapter varies the randomization format for gradient descent where Gaussian noise is injected at each iteration step. We point out the ergodicity property of such variation, which is not available for a deterministic version of gradient descent. The third chapter considers using the sketching technique to compress data and evaluate statistics solely based on the sketched dataset [HB20b]. We give theoretical guarantee for the evaluation accuracy of autocorrelation from data sketches and demonstrate numerical performance on molecular dynamic simulation data and synthetic data. The fourth chapter considers a deterministic algorithm for integer-constrained programming, where we suggest a finer convex relaxation where the primal problem is reformulated by Fenchel-Rockafellar duality, and separated into two subproblems based on pre-selected support. </details>
- [Iterative stochastic optimization for large-scale machine learning and statistical inverse problems](https://repository.mines.edu/handle/11124/174167), David Kozak, 2020, PhD (main advisor: Luis Tenorio, Colorado School of Mines, Applied Math & Stat)
- [Stokes, Gauss, and Bayes walk into a bar...](https://scholar.colorado.edu/appm_gradetds/144/), Eric Kightley, 2019, PhD
- [Non-Convex Optimization and Applications to Bilinear Programming and Super-Resolution Imaging](https://scholar.colorado.edu/appm_gradetds/142/), Jessica Gronski, 2019, PhD
- [Fast and Reliable Methods in Numerical Linear Algebra, Signal Processing, and Image Processing](https://scholar.colorado.edu/appm_gradetds/134/), James Folberth, 2018, PhD
- [Randomized Algorithms for Large-Scale Data Analysis](https://scholar.colorado.edu/ecen_gradetds/141/), Farhad Pourkamali Anaraki, 2017, PhD

Masters theses
- [Long Short-Term Memory Networks to Improve Aerodynamic Coefficient Estimation for Aerocapture](/assets/docs/DominicRudakevych_MSthesis_2025), Dominic Rudakevych, Professional Masters program / Draper Scholar, 2025  <details> Aerocapture is a method for orbital insertion from a hyperbolic trajectory being considered for NASA’s proposed 2030’s flagship mission to Uranus. By traveling through the planet's atmosphere to generate drag, aerocapture greatly reduces the fuel needed when firing retrograde thrusters, allowing for larger payloads or a less powerful launch vehicle. Despite thes theoretical benefits, aerocapture has never flown on any planetary missions due to thin margin of error and in-situ corrections necessary to properly execute the maneuver. Critical to the guidance and control algorithms are the aerodynamic coefficients. We propose using a neural network to learn the nonlinear relationship between the raw sensor data and these aerodynamic coefficients. Specifically, we explore how network architectures designed for time dependent data, like Long-Short Term Memory (LSTM) neural networks, can produce aerodynamic coefficient estimates akin to that of a computational fluid dynamics (CFD) based lookup table, while providing more robust coefficient estimation when large environmental perturbations ar experienced. Improving force and moment coefficient estimation would improve aerocapture by providing more accurate aerodynamic coefficients for use in guidance and control algorithms. This work considers multiple sensed data sources and aerodynamic coefficient data along with LSTM network architectures for model training to maximize an aerocapture maneuver’s success rate when tested in a Monte Carlo simulation. Along with this, sensitivity analyses were conducted on model hyperparameters to account for relationship complexity. Results were compared against traditional aerodynamic coefficient lookup tables within the Fully Numeric Predictor-corrector Aerocapture Guidance (FNPAG) algorithm to draw conclusions for the model’s performance. </details>
- [A simple randomized algorithm for approximating the spectral norm of streaming data](/assets/docs/SpencerShortt_Masters2023.pdf), Spencer Shortt, 2023 (Math dept, MS presentation)
- [Regularized Saddle-Free Newton: Saddle Avoidance and Efficient Implementation](/assets/docs/CooperSimpson_Summer2022_MAthesis.pdf), Cooper Simpson, 2022, Masters <details> We present a new second-order method for unconstrained non-convex optimization, which we dub Regularized Saddle-Free Newton (R-SFN). This work builds upon a number of recent ideas related to improving the practical performance of the classic Newton’s method. In particular, we develop a nonlinear
transformation to the Hessian which ensures it is positive definite at each iteration by approximating the matrix absolute value and regularizing with a scaled gradient norm. While our method applies to C2 objectives with Lipschitz Hessian, our analysis will require the existence of a third continuous derivative. Given this, we show that with an appropriately random initialization our method avoids saddle points almost surely. Furthermore, the form of our nonlinear transformation facilitates an efficient matrix-free approach to computing the update via Krylov based quadrature, making our method scalable to high dimensional problems. </details>
- [Stochastic Lanczos Likelihood Estimation of Genomic Variance Components](https://scholar.colorado.edu/appm_gradetds/120/), Richard Border, 2018, Masters
- [A comparison of spectral estimation techniques](https://scholar.colorado.edu/appm_gradetds/147/), Marc Thomson, 2019, Masters; code at [github](https://github.com/MarcThomson/MDSpectralAnalysis).
- [Optimization for High-Dimensional Data Analysis](https://scholar.colorado.edu/appm_gradetds/86/), Derek Driggs, 2017, Masters

Undergraduate theses
- [Improving Existing Bayesian Optimization Software by Incorporating Gradient Information](https://github.com/yyexela/botorch/blob/main/tutorials/fobo.ipynb), Alexey Yermakov (CS and APPM), CS Capstone senior thesis, 2023 <details>This tutorial demonstrates how to use gradient information in Bayesian Optimization. Traditionally, Bayesian Optimization is a zeroth-order method that does not utilize gradient information because oftentimes the black-box functions being optimized does not provide gradients. However, there may be some cases where gradient information is available [1]. As a result, derivative-enabled Bayesian Optimization may use the additional gradient information. We will call zeroth-order Bayesian Optimization ZOBO and first-order Bayesian Optimization FOBO. </details>
- [A Generalization of S-Divergence to Symmetric Cone Programming via Euclidean Jordan Algebra (local copy)](/assets/docs/Jaden_Wang_2022_thesis.pdf), Zhuochen (Jaden) Wang, 2022, BS <details> Symmetric cone programming encompasses a vast majority of tractable convex optimization problems, including linear programming, semidefinite programming, and second-order cone programming. It turns out that we can generalize many results from semidefinite matrices to symmetric cones under the abstract framework of Euclidean Jordan algebra. In particular, S-divergence was previously proposed as a numerical alternative to Riemannian distance for the Hermitian positive definite cone. The goal of this thesis is to generalize S-divergence to symmetric cones and prove that its nice properties in the matrix case are preserved. Specifically, we wish to show that S-divergence induces a metric on the cone and is geodesically-convex. After an extensive exposition of necessary background, we successfully proved most of our claims, with only one conjecture remaining to be proven. </details>

Professional Masters "culminating experience"
- Austin Wagenknecht, 2022, Digital Signal Processing Methods in Music Information Retrieval
- Jacob Tiede, 2021, [culminating experience](https://github.com/Jacob-Tie/GraduateSchoolCourseWork/blob/master/Capstone_Project/Article.md)

## Miscellany

- [Open Problem: Property Elicitation and Elicitation Complexity](http://www.jmlr.org/proceedings/papers/v49/frongillo16.html), Rafael Frongillo, Ian Kash, Stephen Becker. 29th Annual Conference on Learning Theory (COLT), pp. 1655–1658, 2016
- [Trig identities](/assets/docs/trig.pdf) that I used in high school and college

