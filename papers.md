---
layout: page_with_toc
title: Papers
permalink: /papers/
---

For the very latest, see autogenerated lists, like:
- [Stephen's arXiv papers](http://arxiv.org/a/becker_s_1)
- [google scholar](http://scholar.google.com/citations?hl=en&user=fUDbcnYAAAAJ)
- [DBLP](https://dblp.org/pid/47/9100) computer science papers
- [ResearchGate](https://www.researchgate.net/profile/Stephen_Becker5) and [academia.edu](https://colorado.academia.edu/StephenBecker)
- [ORCID](http://orcid.org/0000-0002-1932-8159)
- [Semantic Scholar](https://www.semanticscholar.org/author/50333204)

<!-- see https://github.com/jwrr/minima-sidebar/blob/master/sidebar-fixed.markdown -->
<!--
<nav class="toc-fixed" markdown="1">
* TOC
{:toc}
</nav>
-->

Click the title or triangle to show *abstracts*

<!-- See https://jekyllrb.com/docs/datafiles/ 
and https://github.com/Shopify/liquid/wiki/Liquid-for-Designers 

There's a secret special value empty (unquoted) that you can compare arrays to; the comparison is true if the array has no publications.

Liquid expressions are tested for "truthiness" in what looks like a Ruby-like way:

true is true.
false is false.
Any string is true, including the empty string.
Any array is true.
Any hash is true.
Any nonexistent/nil value (like a missing publication of a hash) is false.

In Liquid, you can include a hyphen in your tag syntax using minus sign, see https://stackoverflow.com/questions/35642820/jekyll-how-to-use-for-loop-to-generate-table-row-within-the-same-table-inside-m

-->
## Preprints {#preprints}
<ul class="papers" id="preprints">
{% for publication in site.data.papers %}
{% if publication.unpublished == true %}
  <div class="selection"> <details class="papers" id="{{ publication.id }}"><summary>{{ publication.year }} <span class="title">{{ publication.title }}</span></summary><b>Abstract</b>: {{ publication.abstract }}</details>
  <ul> <li>
<span class="author">  {{ publication.author }}.</span> 
<span class="venue">{{ publication.venue }}</span><span class="pubMisc">{{ publication.misc }}</span>
{% if publication.DOI != null %}
[<a href="{{ publication.DOI }}">DOI</a>]
    {% endif %}

    {% if publication.arXiv != null %}
    [<a href="https://arxiv.org/abs/{{ publication.arXiv | string }}">arXiv</a>]
    {% endif %}

    {% if publication.code != null %}
    [<a href="{{ publication.code }}">code</a>]
    {% endif %}

    {% if publication.other-link != null %}
    {{ publication.other-link }}
    {% endif %}

    </li></ul></div>
{% endif %}
{% endfor %}
</ul>

## Journal papers {#journals}
<ul class="papers" id="journals">

<!-- First, find out total # of papers so I can count backwards -->
{% assign number_journal_papers = 0 %}
{% assign number_conference_papers = 0 %}
{% for publication in site.data.papers %}
{% if publication.unpublished == null or publication.unpublished == false %}
{% if publication.conference == null or publication.conference == false %}
{% assign number_journal_papers = number_journal_papers | plus: 1 %}
{% else %}
{% assign number_conference_papers = number_conference_papers | plus: 1 %}
{% endif %}
{% endif %}
{% endfor %}

{% assign current_paper = number_journal_papers %}
{% for publication in site.data.papers %}
{% if publication.conference == null or publication.conference == false %}
{% if publication.unpublished == null or publication.unpublished == false %}
   <div class="selection"><details class="papers" id="{{ publication.id }}"><summary>[J{{current_paper}}] {{ publication.year }} <span class="title">{{ publication.title }}</span></summary><b>Abstract</b>: {{ publication.abstract }}</details>
  <ul> <li>
<span class="author">  {{ publication.author }}.</span> 
<span class="venue">{{- publication.venue -}}</span><span class="pubMisc">{% if publication.misc != null %}, {{ publication.misc -}}.
<!-- <em>{{ publication.venue -}}</em> -->
<!-- test if fields exist, so that I don't get extra commas -->
{%- else -%}.
{% endif %}</span>
{% assign current_paper = current_paper | minus: 1 %}
    {% if publication.DOI != null %}
    [<a href="{{ publication.DOI }}">DOI</a>]
    {% endif %}

    {% if publication.arXiv != null %}
    [<a href="https://arxiv.org/abs/{{ publication.arXiv }}">arXiv</a>]
    {% endif %}

    {% if publication.code != null %}
    [<a href="{{ publication.code }}">code</a>]
    {% endif %}

    {% if publication.other-link != null %}
    {{ publication.other-link }}
    {% endif %}
    </li></ul></div>
{% endif %}
{% endif %}
{% endfor %}
</ul>

## Conference papers {#conference}
<ul class="papers" id="conference">
{% assign current_paper = number_conference_papers %}
{% for publication in site.data.papers %}
{% if publication.conference == true %}
{% if publication.unpublished == null or publication.unpublished == false %}
   <div class="selection"><details class="papers" id="{{ publication.id }}"><summary>[C{{current_paper}}] {{ publication.year }} <span class="title">{{ publication.title }}</span></summary><b>Abstract</b>: {{ publication.abstract }}</details>
  <ul> <li >
<span class="author">  {{ publication.author }}.</span> 
<span class="venue">{{- publication.venue -}}</span><span class="pubMisc">{% if publication.misc != null %}, {{ publication.misc -}}.{%- else -%}.
{% endif %}</span>
        {% assign current_paper = current_paper | minus: 1 %}

    {% if publication.DOI != null %}
    [<a href="{{ publication.DOI }}">DOI</a>]
    {% endif %}

    {% if publication.arXiv != null %}
    [<a href="https://arxiv.org/abs/{{ publication.arXiv }}">arXiv</a>]
    {% endif %}

    {% if publication.code != null %}
    [<a href="{{ publication.code }}">code</a>]
    {% endif %}

    {% if publication.other-link != null %}
    {{ publication.other-link }}
    {% endif %}
    </li></ul></div>
{% endif %}
{% endif %}
{% endfor %}
</ul>

## Book chapters {#chapters}
- Dual Smoothing Techniques for Variational Matrix Decomposition, S. Becker and A. Aravkin, in “Robust Low-Rank and Sparse Matrix Decomposition: Applications in Image and Video Processing”, T. Bouwmans, N. Aybat, E. Zahzah, eds. CRC Press, 2016. [arXiv](http://arxiv.org/abs/1603.00284)

## Selected Talks {#talks}

Here I list a few talks that have video or slides or websites

- "High-dimensional DFO: Stochastic Subspace Descent and improvements", ICCOPT, USC, Los Angeles, July 23 2025. [Slides](/assets/docs/2025_ICCOPT_USC.pdf)
- "Research in Applied Math at CU", Math Circles talk (K-12), Sept 15 2024.  [Slides](/assets/docs/MathCircles_Sept15_2024.pdf).
- "Randomization methods for big-data", at CU CS Dept colloquium Oct 26 2023, and Purdue Math dept Applied Math seminar Oct 23 2023.  [Slides](/assets/docs/2023_10_CU_CS_RandomMethods.pdf).
  - For related sketching code:
  - [Random sketching operators](https://github.com/stephenbeckr/randomized-algorithm-class/tree/master/Code) in Matlab and Python, as part of the class I teach. This has Gaussian sketch, Fast Johnson Lindenstrauss (with FFT or a fast Hadamard), and count sketch. The Hadamard and count sketch have C functions to make them extra fast.  
  - For the tensor product sketches, which is more complicated, see [Osman's github page](https://github.com/OsmanMalik/)
- "High-Probability convergence and algorithmic stability for stochastic gradient descent", [ML-MTP: Machine Learning in Montpellier: Theory and Practice](https://groupes.renater.fr/wiki/ml-mtp/index) seminar, September 2022. [Slides](/assets/docs/2022_Sept_IMAG.pdf).  Similar versions given in Arcachon (Curves and Surfaces, June 2022) and Nice (October 2022)
- "Stochastic Subspace Descent: Stochastic gradient-free optimization, with applications to PDE-constrained optimization"   [Slides (2.7 MB pdf)](/assets/docs/2022_ICCOPT_Lehigh_SSD_Becker.pdf), at ICCOPT, Lehigh University July 2022.  Similar version given January 2020 in Denver (joint MAA meetings).
- "Optimization for statistical estimators: Applications to quantum fidelity estimation" [Slides (3.7 MB pdf)](/assets/docs/2022_Stockholm_Becker_midRes.pdf), at Conference on the Mathematics of Complex Data, KTH Royal Institute of Technology, Stockholm. June 13-16 2022
- "Matrix Completion and Robust PCA", University of Colorado Boulder, Computer Science department colloquium, Boulder, CO. Nov 20 2014. [video](http://vimeo.com/channels/cucs)
- [Advances in first-order methods: constraints, non-smoothness and faster convergence](
https://fadili.users.greyc.fr/siamis12/minisymposium2.html), Minisymposium, SIAM Imaging Science, Philadelphia. [Slides](https://fadili.users.greyc.fr/siamis12/minisymposium2/BeckerIS12.pdf), May 22 2012
- ["TFOCS: Flexible First-order Methods for Rank Minimization"](/assets/docs/SIOPT2011_Becker.pdf), at the [Low-rank Matrix Optimization minisymposium](http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=12168) at the 2011 SIAM Conference on Optimization in Darmstadt. Here's the [RPCA video](http://www.youtube.com/watch?v=Yxj1_52EAXA) (Matlab code to generate this can be found at the [Demo page on the TFOCS website](http://cvxr.com/tfocs/demos/rpca/).  May 19 2011
- [Quick intro to convex optimization](/assets/docs/convexOptTalk.pdf) talk for Patrick Sanan's department "ACM^tea", Oct 23 2009

## Technical Reports {#techReports}
- [Variational Entropy Search for Adjusting Expected Improvement](https://arxiv.org/abs/2402.11345), Nuojin Cheng, Stephen Becker, 2024. Superseded by [A Unified Framework for Entropy Search and Expected Improvement in Bayesian Optimization](https://arxiv.org/abs/2501.18756), Nuojin Cheng, Leonard Papenmeier, Stephen Becker, Luigi Nardi, 2025.
- [ Locality-sensitive hashing in function spaces](https://arxiv.org/abs/2002.03909), Will Shand and Stephen Becker, 2020
- [Tensor Robust Principal Component Analysis: Better recovery with atomic norm regularization](https://arxiv.org/abs/1901.10991), Derek Driggs, Stephen Becker and Jordan Boyd-Graber, 2019
- [URV Factorization with Random Orthogonal System Mixing](https://arxiv.org/abs/1703.02499), Stephen Becker, James Folberth, Laura Grigori, 2017
- [The Chen-Teboulle algorithm is the proximal point algorithm](https://arxiv.org/abs/1908.03633), from 2011 but posted 2019, shows the Chen-Teboulle algorithm admits a more aggressive stepsize than via the original analysis.
- [Exact linesearch for LASSO](/assets/docs/exactLinesearchL1.pdf) discusses exact step-size selection for piecewise quadratic objective functions, with [code](https://github.com/stephenbeckr/exactLASSOlinesearch). 2016.


## Theses {#theses}

- 2011 [Practical Compressed Sensing: modern data acquisition and signal processing](http://resolver.caltech.edu/CaltechTHESIS:06022011-152525054), California Institute of Technology (PhD thesis). Co-winner, Carey prize. [local PDF](/assets/docs/Becker_thesis.pdf) <details> Since 2004, the field of compressed sensing has grown quickly and seen tremendous interest because it provides a theoretically sound and computationally tractable method to stably recover signals by sampling at the information rate. This thesis presents in detail the design of one of the world's first compressed sensing hardware devices, the random modulation pre-integrator (RMPI). The RMPI is an analog-to-digital converter (ADC) that bypasses a current limitation in ADC technology and achieves an unprecedented 8 effective number of bits over a bandwidth of 2.5 GHz. Subtle but important design considerations are discussed, and state-of-the-art reconstruction techniques are presented. Inspired by the need for a fast method to solve reconstruction problems for the RMPI, we develop two efficient large-scale optimization methods, NESTA and TFOCS, that are applicable to a wide range of other problems, such as image denoising and deblurring, MRI reconstruction, and matrix completion (including the famous Netflix problem). While many algorithms solve unconstrained l1 problems, NESTA and TFOCS can solve the constrained form of l1 minimization, and allow weighted norms. In addition to l1 minimization problems such as the LASSO, both NESTA and TFOCS solve total-variation minimization problem. TFOCS also solves the Dantzig selector and most variants of the nuclear norm minimization problem. A common theme in both NESTA and TFOCS is the use of smoothing techniques, which make the problem tractable, and the use of optimal first-order methods that have an accelerated convergence rate yet have the same cost per iteration as gradient descent. The conic dual methodology is introduced in TFOCS and proves to be extremely flexible, covering such generic problems as linear programming, quadratic programming, and semi-definite programming. A novel continuation scheme is presented, and it is shown that the Dantzig selector benefits from an exact-penalty property. Both NESTA and TFOCS are released as software packages available freely for academic use. </details>

- 2005 [Translational and Rotational Dynamics of Supercooled Water](/assets/docs/thesis.pdf), Wesleyan University (undergraduate thesis). Co-winner, Bertman prize. <details> This thesis presents the results of extensive simulations of the ST2 model of water designed to explore both thermodynamic and dynamic properties in super- cooled states. Constant volume simulations were performed at 1,233 state points, with densities ranging from 0.80 g/cm3 to 1.20 g/cm3 in steps of .01 g/cm3; along these isochores, simulations were run at temperatures from 250 K to 400 K in 5 K intervals. Our results qualitatively reproduce many of the expected properties of supercooled water such as the density anomaly and a novel low-temperature phase transition between two liquid states. The Stokes-Einstein and Debye-Stokes- Einstein equations provide a simple hydrodynamic relation between viscosity and diffusion. These relations hold for simple liquids, but are known to fail near a glass transition. For ST2 water, we find that the Debye-Stokes-Einstein equation does not hold at low temperatures. Furthermore, the data indicate that rotational diffusion is enhanced at low temperatures relative to translational diffusion. We also uncover an unexpected connection between dynamics and thermodynamics; specifically, we find that the structural relaxation time τα along the critical iso-chore provides a precursor to the liquid-liquid phase transition at temperatures up to 150 K above the critical temperature. We observe a nearly identical sig- nature in τα at a higher density which may indicate a second liquid-liquid phase transition, adding credibility to the recent idea that there may be more than one liquid-liquid critical point. </details>

### Student theses
Theses of my students (after 2019, CU stopped posting these online, so I'm posting local copies when I have them):

In the pipeline
- K. Aditi (PhD), 202?
- Nic Rummel (PhD), 202?
- Drona Khurana (PhD), 202?, co-advised with Raf Frongillo (CS)
- Mohit Garg (PhD), 202?, co-advised with Alireza Doostan (Aerospace)
- Cooper Doe (PhD), 202?

PhD theses
- Alex McManus, 2025, PhD (main advisor: Nicholas Dwork, CU Anschutz)
- [Multi-fidelity Uncertainty Quantification and Optimization](/assets/docs/NuojinCheng_PhDthesis_2025.pdf), Nuojin Cheng, 2025, PhD (main advisor: Alireza Doostan, Aerospace). <details> This thesis presents four novel bi-fidelity modeling approaches designed to enhance computational e!ciency and accuracy in uncertainty quantification and optimization. First, Bi-fidelity Boosting (BFB) introduces an e”ective sketching-based subsampling method, accompanied by theoretical analysis of how inter-model correlation impacts performance. Second, the Bi-fidelity Variational Auto-encoder (BF-VAE) leverages deep generative models and transfer learning to achieve high performance with minimal high-fidelity data, also revealing connections between multi-fidelity learning and information bottleneck theory. Third, Langevin Bi-fidelity Importance Sampling (L-BF-IS) develops an efficient score-based Metropolis-Hastings importance sampling estimator for uncertainty quantification, whose e”ectiveness is linked to the discrepancy between model failure probability measures. Finally, a bi-fidelity zero-order optimization framework employs local multi-fidelity surrogates and an Armijo-based line search for optimal step sizes, demonstrating strong empirical performance supported by theoretical convergence guarantees under specific conditions. Collectively, these contributions advance multi-fidelity modeling by providing efficient, theoretically grounded methods for tackling complex computational challenges. </details>
- [Computational Methods of Optimization and Geometry Processing for Physics Applications](/assets/docs/JacobSpainhour_PhDthesis_2025.pdf), Jacob Spainhour, 2025, PhD. <details> 
This thesis presents novel computational methods in two distinct, yet not disjoint subfields of applied mathematics, and is consequently divided into two parts. In Part I, we consider methods of numerical optimization as applied to problems in scientific experimentation, in which a limited quantity of available resources must be carefully allocated to maximize the performance of the experiment. In the first chapter of Part I, this takes the for of discovering optimized ultrasound transmission sequences which, when imaged in the REFoCUS framework, produce high quality images. In the second chapter of Part I, this takes the form of discovering optimized measurement protocols for the verification of quantum states through fidelity estimation.
In Part II, we consider methods of computational geometry as applied to problems in multiphysics simulation, in which geometrically complex objects must be computationally handled in a way that is reliable and robust to numerical imperfection. In the first and second chapter of Part II, we develop methods of evaluating the generalized winding number for general curves and surfaces, from which we derive methods of determining containment for geometric objects which otherwise do not define an interior volume. In the third chapter of Part II, we develop a method of 3D material interface reconstruction by means of optimally defining a collection of half-spaces whose convex intersection has geometric moments as close as possible to provided reference data. </details>
- [Minimax Optimal Estimation of Expectation Values](/assets/docs/AkshaySeshadri_PhDthesis_2025.pdf), or [proquest version](https://www.proquest.com/openview/7848b3068937cadd9e398bb92444fa2a/) Akshay Seshadri, 2025, PhD (co-advised with Emanuel Knill, NIST) <details>  There is growing interest in constructing quantum devices that control increasingly large numbers of quantum systems. There is a tremendous need for methods that can measure the quality and other properties of these devices. Such measurements often amount to learning the expectation value of a quantum observable with respect to the quantum state of the device. Learning expectation values is also a key component in many well-known applications such as quantum machine learning and quantum optimization algorithms. Because of the large system sizes involved, it is essential to find methods for learning expectation values that are e!cient with respect to the system size. Existing methods that have rigorous guarantees and are practical to implement are often not optimal for observables of interest. Other methods are heuristic, or require the use measurement protocols that are challenging to implement experimentally with current technology. In this study, we propose an estimation procedure, The Optimal Observable expectation valueLearner or TOOL, that can learn the expectation values of observables using the outcomes of any given measurement protocol. We show that there is a seminorm on the set of all observables, which we call the minimax norm, that characterizes the smallest possible estimation error for learning a given observable using the outcomes of a given non-adaptive measurement protocol to a specified confidence level in the worst case over all states. We prove that TOOL is minimax optimal for every observable by showing that it can achieve an estimation error to within a small constant factor of the minimax norm. For many applications, one wishes to learn the expectation value of more than one observable from the same experiment. A popular method for learning the expectation values of one or many observables with rigorous guarantees is classical shadows. Classical shadows has near-optimal performance in the worst case over all observables. We prove that TOOL always performs at least as well as classical shadows. Moreover, we show by example that TOOL dramatically outperform classical shadows for many observables of interest. This highlights the need to characterize the optimal performance for the task of simultaneously learning the expectation values of many observables. Under a mild assumption, we give such a characterization using the minimax norm and prove thatTOOL is nearly minimax optimal for this task. We also study the applications of TOOL to fidelity estimation. Using experimental data from a trapped-ion quantum computer, we show that TOOL performs well in practice and matches the estimates obtained from Maximum Likelihood Estimation (MLE), but with rigorous guarantees on the estimation error unlike MLE. We also compare TOOL with another popular method called direct fidelity estimation, which estimates the fidelity by judiciously sampling Pauli observables and measuring them. We show that there is a di”erent importance sampling scheme for Pauli measurements for which TOOL performs as well as, or better than, direct fidelity estimation. Since TOOL constructs an estimator using only the observable, the measurement protocol, and the confidence level, it provides the flexibility to perform estimation for experiments that have already been performed and experiments that will be performed in the future. Similarly, since the minimax norm can be computed beforehand, it can be used to compare the performance of different measurement protocols and allow minimax optimal design of experiments. </details>
- [Numerical Methods for Non-uniform Data Sources](https://www.proquest.com/docview/3100721110/FF9F64101B99442DPQ), Kevin Doherty, 2024, PhD. [local PDF copy](/assets/docs/KevinDoherty_thesis2024.pdf) <details> This thesis surveys and creates methods to allow for a mathematically consistent treatment of non-uniform data sources in machine learning and data compression. These methods are fundamentally rooted in numerical methods for quadrature and interpolation, but are leveraged with appropriate computational tools and techniques to adapt to their respective domains and problem types. </details>
- [Mathematical Formulations with Uncertain Data in
Optimization and Inverse Problems](https://www.rjclancy.com/assets/documents/dissertation.pdf), Richard Clancy, 2022, PhD <details> This dissertation broadly focuses on incorporating uncertainty into mathematical models and
can be broken into three distinct sections. In the first (Ch. 2 and 3), we consider design matrix uncertainty for linear regression. The motivation is that many regressors or covariates used to build linear models, such as survey responses and other data subjective in nature, are intrinsically uncertain. We approach the problem from two angles: 1) using robust optimization and 2) an approximation method to construct an otherwise cumbersome objective for maximum likelihood estimation. Expressions for (sub)gradients are provided allowing for the use of efficient solvers. We illustrate the merit of both methods on synthetic data. In the second section (Ch. 4 and 5), we focus on novel trust region methods that accept uncertainty in objective and gradient evaluations to reduce the computational burden. We introduce an algorithm named TROPHY (Trust Region Optimization using a Precision HierarchY) which uses variable precision arithmetic to reduce communication, storage, and energy costs. The other project uses a Hermite interpolation based framework to build model objectives using function and derivative information. Since not all partial derivatives are available or are expensive to compute, we make use of uncertain gradients to improve performance beyond standard interpolation methods. We also propose the use of sketching to ameliorate issues with redundant data and reduce the burden of inverting very large matrices. The final section (Ch. 6) investigates how operator uncertainty impacts the ability to solve inverse problems accurately. Our goal is to localize regions of brain activity using optically pumped magnetometers which are novel sensors that show promise for use in magnetoencephalography. Through a series of simulations, we establish guidelines for sensor count, noise level, and forward model fidelity needed to localize brain activity to a specified accuracy. </details>
- [First-Order Methods for Online and Stochastic Optimization, and Approximate Compiling](https://www.proquest.com/docview/2669410131/310C41AC99D04956PQ),
Liam Madden, 2022, PhD (co-advised with Emiliano Dall'Anese, CU ECEE). [local PDF copy](/assets/docs/LiamMadden_thesis2022.pdf) <details> The oracle complexity model for optimization has been the source of many advances in
optimization algorithms with applications to machine learning, artificial intelligence, and highdimensional statistics. For example, many adaptive methods have their beginnings as algorithms
with tune-able hyper-parameters appearing in theoretical complexity bounds. This thesis investigates three different problems from the oracle complexity model.
The first problem, presented in Chapter 2, considers the online smooth convex optimization
problem, motivated by time-varying applications. Two important results are a lower bound on the upper bounds of general online first-order methods and an algorithm with a matching upper bound. The algorithm presented applies Nesterov’s method to a stale cost function until sufficient convergence has been achieved at which point the cost function is updated to the current one.
The second problem, presented in Chapter 3, considers the approximate compiling problem of quantum computing. We focus on the CNOT+rotation gate set and consider three different CNOT patterns. Despite the non-convexity of the problem, we find that local minima perform well for random target circuits. We also find that simple CNOT patterns seem to achieve the lower bound on the number of CNOTs required to exactly compile random target circuits. We also use the optimization framework to explore and find new decompositions of particular target circuits such as the Toffoli gate.
The third problem, presented in Chapter 4, considers the stochastic smooth non-convex optimization problem, motivated by machine learning applications. Two important results are a Freedman-type concentration inequality that breaks through the sub-exponential threshold to heavier-tailed martingale difference sequences and a high probability convergence bound for stochastic gradient descent with sub-Weibull gradient noise. </details>
- [Measuring Image Resolution in Super-Resolution Microscopy and Bayesian Estimation of Population Size and Overlap and Vaccine Effectiveness](https://www.proquest.com/docview/2572564388/3313CC654D214068PQ), Erik Johnson, 2021, PhD (main advisor Dan Larremore, CU CS) <details> This thesis is a compilation of three separate projects. Although the three projects are quite distinct in that they do not directly build on one another, the common thread is statistics and computation applied to medicine and biology. In the first project, we develop a Bayesian statistical model to quantify the size of and overlap between two populations from subsamples from each population. Our model produces joint estimates of the size of each population and the amount of overlap between the populations. We apply our model to compare the genetic repertoires of malaria parasites. In particular, we use our method to construct parasite genetic similarity networks with which we analyze the evolution and transmission of malaria in Ecuador before and after an outbreak in 2012-2013. We also use our method’s inferences to argue that the polymerase chain reaction primers used to study an important malaria parasite gene are biased. In the second project, motivated by the COVID-19 pandemic, we develop a Bayesian statistical model to estimate vaccine effectiveness using data from a common vaccine effectiveness study design called a test-negative design. Since disease positivity is typically assessed using diagnostic tests that have imperfect sensitivity and specificity, our model adjusts for imperfect diagnostic tests along with other potential confounders. In addition, our framework allows data streams from multiple diagnostic test types to be rigorously combined which makes our model flexible for analyses and meta-analyses of test-negative designs. In the third project, we study image resolution metrics that are applicable to modern super-resolution fluorescent microscopy techniques. After showing that the primary existing quantitative image resolution metric called Fourier ring correlation is not ideal, we argue that image resolution in super-resolution microscopy might best be analyzed using information theory. Towards that end, we propose a mutual information-based image resolution metric and we study our metric’s behavior through a few simple examples. </details>
- [Topics in Matrix and Tensor Computations](https://www.proquest.com/docview/2572559828/2D96D8173E184A7CPQ/), Osman Malik, 2021, PhD. [local PDF copy](/assets/docs/OsmanMalik_thesis2021.pdf) <details> This dissertation looks at matrices and tensors from a computational perspective. An important problem in both matrix and tensor computations is decomposition. Several chapters in this dissertation deal with this problem. Chapter 2 develops randomized algorithms for Tucker tensor decomposition which are faster and can handle larger datasets than competing methods. These algorithms are the first ever one-pass methods for Tucker decomposition to appear in the literature. Chapter 4 develops randomized algorithms with guarantees for both matrix and tensor interpolative decomposition (ID). A key contribution is the first ever theoretical guarantees for any randomized tensor ID algorithm. Chapter 7 develops a randomized algorithm for the tensor ring decomposition. By using leverage score sampling, the resulting iterative algorithm has a per iteration cost which is sublinear in the number of input tensor entries. Chapter 8 considers the binary matrix factorization problem and develops a QUBO formulation for solving it on special purpose hardware like the Fujitsu Digital Annealer and the D-Wave Quantum Annealer. In the tensor decomposition works mention above, randomization—more specifically, matrix sketching—is a key tool used to make algorithms faster and more efficient. In Chapter 3 we dive deeper into this topic and consider one such sketch, the Kronecker fast Johnson–Lindenstrauss transform (KFJLT). We provide a novel proof that the KFJLT indeed is a Johnson–Lindenstrauss transform. Randomization is also used in Chapter 5 to improve the performance of fast matrix multiplication algorithms plagued by either numerical rounding error or error in the computation formula itself. An upshot of this work is a simple method that can help make fast algorithms such as Strassen’s more robust, especially on low-precision hardware like GPUs. Machine learning is a field where tensor methods have received considerable attention. In Chapter 6, we develop a tensor based graph neural network that can be used for a variety of prediction tasks on time varying graphs. We achieve competitive performance in prediction tasks, including in a COVID-19 contact tracing application. </details>
- [Randomization in Statistical Machine Learning](https://www.proquest.com/docview/2447014909/80A21D291D8B4D9BPQ), Zhishen (Leo) Huang, 2020, PhD. [local PDF copy](/assets/docs/ZhishenHuang_thesis2020.pdf) <details> Supervised learning and reinforcement learning problems are often formulated as optimization problems for training. The optimization algorithms themselves bear interest from the mathematical point of view. This thesis discusses the usage of randomization in optimization, which makes possible what corresponding deterministic algorithms are unable to achieve. Applying randomization in algorithms has the capability of reducing the time or space complexity at the expense of potential failure to provide a valid solution. Early prominent examples of randomized algorithms include quicksort, Karger’s algorithm for min-cut problem and the Bloom filter. In this thesis, randomization is introduced into optimization algorithms and is used for data compression. This thesis considers first-order optimization algorithms due to their practicality for large- scale application. The first chapter considers the minimization of nonconvex and nonsmooth objectives, where we give probabilistic guarantees for the proximal gradient descent method to converge to local minima [HB20a]. The second chapter varies the randomization format for gradient descent where Gaussian noise is injected at each iteration step. We point out the ergodicity property of such variation, which is not available for a deterministic version of gradient descent. The third chapter considers using the sketching technique to compress data and evaluate statistics solely based on the sketched dataset [HB20b]. We give theoretical guarantee for the evaluation accuracy of autocorrelation from data sketches and demonstrate numerical performance on molecular dynamic simulation data and synthetic data. The fourth chapter considers a deterministic algorithm for integer-constrained programming, where we suggest a finer convex relaxation where the primal problem is reformulated by Fenchel-Rockafellar duality, and separated into two subproblems based on pre-selected support. </details>
- [Iterative stochastic optimization for large-scale machine learning and statistical inverse problems](https://repository.mines.edu/handle/11124/174167), David Kozak, 2020, PhD (main advisor: Luis Tenorio, Colorado School of Mines, Applied Math & Stat)
- [Stokes, Gauss, and Bayes walk into a bar...](https://scholar.colorado.edu/appm_gradetds/144/), Eric Kightley, 2019, PhD <details> This thesis consists of three distinct projects. The first is a study of microbial aggregate fragmentation, in which we develop a dynamical model of aggregate deformation and breakage and use it to obtain a post-fragmentation density function. The second and third projects deal with dimensionality reduction in machine learning problems. In the second project, we derive a one-pass sparsified Gaussian mixture model to perform clustering analysis on high-dimensional streaming data. The model estimates parameters in dense space while storing and performing computations in a compressed space. In the final project, we build an expert system classifier with a Bayesian network for use on high-volume streaming data. Our approach is specialized to reduce the number of observations while obtaining sufficient labeled training data in a regime of extreme class-imbalance and expensive oracle queries. </details>
- [Non-Convex Optimization and Applications to Bilinear Programming and Super-Resolution Imaging](https://scholar.colorado.edu/appm_gradetds/142/), Jessica Gronski, 2019, PhD <details> Bilinear programs and Phase Retrieval are two instances of nonconvex problems that arise in engineering and physical applications, and both occur with their fundamental difficulties. In this thesis, we consider various methods and algorithms for tackling these challenging problems and discuss their effectiveness. Bilinear programs (BLPs) are ubiquitous in engineering applications, economics, and operations research, and have a natural encoding to quadratic programs. They appear in the study of Lyapunov functions used to deduce the stability of solutions to differential equations describing dynamical systems. For multivariate dynamical systems, the problem formulation for computing an appropriate Lyapunov function is a BLP. In electric power systems engineering, one of the most practically important and well-researched subfields of constrained nonlinear optimization is Optimal Power Flow wherein one attempts to optimize an electric power system subject to physical constraints imposed by electrical laws and engineering limits, which can be naturally formulated as a quadratic program. We study the relationship between data flow constraints for numerical domains such as polyhedra and bilinear constraints. The problem of recovering an image from its Fourier modulus, or intensity, measurements emerges in many physical and engineering applications. The problem is known as Fourier phase retrieval wherein one attempts to recover the phase information of a signal in order to accurately reconstruct it from estimated intensity measurements by applying the inverse Fourier transform. The problem of recovering phase information from a set of measurements can be formulated as a quadratic program. This problem is well-studied but still presents many challenges. The resolution of an optical device is defined as the smallest distance between two objects such that the two objects can still be recognized as separate entities. Due to the physics of diffraction, and the way that light bends around an obstacle, the resolving power of an optical system is limited. This limit, known as the diffraction limit, was first introduced by Ernst Abbe in 1873. Obtaining the complete phase information would enable one to perfectly reconstruct an image; however, the problem is severely ill-posed and the leads to a specialized type of quadratic program, known as super-resolution imaging, wherein one attempts to learn phase information beyond the limits of diffraction and the limitations imposed by the imaging device. </details>
- [Fast and Reliable Methods in Numerical Linear Algebra, Signal Processing, and Image Processing](https://scholar.colorado.edu/appm_gradetds/134/), James Folberth, 2018, PhD <details> In this dissertation we consider numerical methods for a problem in each of numerical linear algebra, digital signal processing, and image processing for super-resolution fluorescence microscopy. We consider first a fast, randomized mixing operation applied to the unpivoted Householder QR factorization. The method is an adaptation of a slower randomized operation that is known to provide a rank-revealing factorization with high probability. We perform a number of experiments to highlight possible uses of our method and give evidence that our algorithm likely also provides a rank-revealing factorization with high probability.  In the next chapter we develop fast algorithms for computing the discrete, narrowband cross-ambiguity function (CAF) on a downsampled grid of delay values for the purpose of quickly detecting the location of peaks in the CAF surface. Due to the likelihood of missing a narrow peak on a downsampled grid of delay values, we propose methods to make our algorithms robust against missing peaks. To identify peak locations to high accuracy, we propose a two-step approach: first identify a coarse peak location using one of our delay-decimated CAF algorithms, then compute the CAF on a fine, but very small, grid around the peak to find its precise location. Runtime experiments with our Cplusplus implementations show that our delay-decimated algorithms can give more than an order of magnitude improvement in overall runtime to detect peaks in the CAF surface when compared against standard CAF algorithms. In the final chapter we study non-negative least-squares (NNLS) problems arising from a new technique in super-resolution fluorescence microscopy. The image formation task involves solving many tens of thousands of NNLS problems, each using the same matrix, but different right-hand sides. We take advantage of this special structure by adapting an optimal first-order method to efficiently solve many NNLS problems simultaneously. Our NNLS problems are extremely ill-conditioned, so we also experiment with using a block-diagonal preconditioner and the alternating direction method of multipliers (ADMM) to improve convergence speed. We also develop a safe feature elimination strategy for general NNLS problems. It eliminates features only when they are guaranteed to have weight zero at an optimal point. Our strategy is inspired by recent works in the literature for l1-regularized least-squares, but a notable exception is that we develop our method to use an inexact, but feasible, primal-dual point pair. This allows us to use feature elimination reliably on the extremely ill-conditioned NNLS problems from our microscopy application. For an example image reconstruction, we use our feature elimination strategy to certify that the reconstructed super-resolved image is unique. </details>
- [Randomized Algorithms for Large-Scale Data Analysis](https://scholar.colorado.edu/ecen_gradetds/141/), Farhad Pourkamali Anaraki, 2017, PhD  <details> Massive high-dimensional data sets are ubiquitous in all scientific disciplines. Extracting meaningful information from these data sets will bring future advances in fields of science and engineering. However, the complexity and high-dimensionality of modern data sets pose unique computational and statistical challenges. The computational requirements of analyzing large-scale data exceed the capacity of traditional data analytic tools. The challenges surrounding large high-dimensional data are felt not just in processing power, but also in memory access, storage requirements, and communication costs. For example, modern data sets are often too large to fit into the main memory of a single workstation and thus data points are processed sequentially without a chance to store the full data. Therefore, there is an urgent need for the development of scalable learning tools and efficient optimization algorithms in today's high-dimensional data regimes.A powerful approach to tackle these challenges is centered around preprocessing high-dimensional data sets via a dimensionality reduction technique that preserves the underlying geometry and structure of the data. This approach stems from the observation that high-dimensional data sets often have intrinsic dimension which is significantly smaller than the ambient dimension. Therefore, information-preserving dimensionality reduction methods are valuable tools for reducing the memory and computational requirements of data analytic tasks on large-scale data sets.Recently, randomized dimension reduction has received a lot of attention in several fields, including signal processing, machine learning, and numerical linear algebra. These methods use random sampling or random projection to construct low-dimensional representations of the data, known as sketches or compressive measurements. These randomized methods are effective in modern data settings since they provide a non-adaptive data-independent mapping of high-dimensional data into a low-dimensional space. However, such methods require strong theoretical guarantees to ensure that the key properties of original data are preserved under a randomized mapping. This dissertation focuses on the design and analysis of efficient data analytic tasks using randomized dimensionality reduction techniques. Specifically, four efficient signal processing and machine learning algorithms for large high-dimensional data sets are proposed: covariance estimation and principal component analysis, dictionary learning, clustering, and low-rank approximation of positive semidefinite kernel matrices. These techniques are valuable tools to extract important information and patterns from massive data sets. Moreover, an efficient data sparsification framework is introduced that does not require incoherence and distributional assumptions on the data. A main feature of the proposed compression scheme is that it requires only one pass over the data due to the randomized preconditioning transformation, which makes it applicable to streaming and distributed data settings. The main contribution of this dissertation is threefold: (1) strong theoretical guarantees are provided to ensure that the proposed randomized methods preserve the key properties and structure of high-dimensional data; (2) tradeoffs between accuracy and memory/computation savings are characterized for a large class of data sets as well as dimensionality reduction methods, including random linear maps and random sampling; (3) extensive numerical experiments are presented to demonstrate the performance and benefits of our proposed methods compared to prior works.  </details>

Masters theses
- [Long Short-Term Memory Networks to Improve Aerodynamic Coefficient Estimation for Aerocapture](/assets/docs/DominicRudakevych_MSthesis_2025.pdf), Dominic Rudakevych, Professional Masters program / Draper Scholar, 2025  <details> Aerocapture is a method for orbital insertion from a hyperbolic trajectory being considered for NASA’s proposed 2030’s flagship mission to Uranus. By traveling through the planet's atmosphere to generate drag, aerocapture greatly reduces the fuel needed when firing retrograde thrusters, allowing for larger payloads or a less powerful launch vehicle. Despite thes theoretical benefits, aerocapture has never flown on any planetary missions due to thin margin of error and in-situ corrections necessary to properly execute the maneuver. Critical to the guidance and control algorithms are the aerodynamic coefficients. We propose using a neural network to learn the nonlinear relationship between the raw sensor data and these aerodynamic coefficients. Specifically, we explore how network architectures designed for time dependent data, like Long-Short Term Memory (LSTM) neural networks, can produce aerodynamic coefficient estimates akin to that of a computational fluid dynamics (CFD) based lookup table, while providing more robust coefficient estimation when large environmental perturbations ar experienced. Improving force and moment coefficient estimation would improve aerocapture by providing more accurate aerodynamic coefficients for use in guidance and control algorithms. This work considers multiple sensed data sources and aerodynamic coefficient data along with LSTM network architectures for model training to maximize an aerocapture maneuver’s success rate when tested in a Monte Carlo simulation. Along with this, sensitivity analyses were conducted on model hyperparameters to account for relationship complexity. Results were compared against traditional aerodynamic coefficient lookup tables within the Fully Numeric Predictor-corrector Aerocapture Guidance (FNPAG) algorithm to draw conclusions for the model’s performance. </details>
- [A simple randomized algorithm for approximating the spectral norm of streaming data](/assets/docs/SpencerShortt_Masters2023.pdf), Spencer Shortt, 2023 (Math dept, MS presentation)
- [Regularized Saddle-Free Newton: Saddle Avoidance and Efficient Implementation](/assets/docs/CooperSimpson_Summer2022_MAthesis.pdf), Cooper Simpson, 2022, Masters <details> We present a new second-order method for unconstrained non-convex optimization, which we dub Regularized Saddle-Free Newton (R-SFN). This work builds upon a number of recent ideas related to improving the practical performance of the classic Newton’s method. In particular, we develop a nonlinear
transformation to the Hessian which ensures it is positive definite at each iteration by approximating the matrix absolute value and regularizing with a scaled gradient norm. While our method applies to C2 objectives with Lipschitz Hessian, our analysis will require the existence of a third continuous derivative. Given this, we show that with an appropriately random initialization our method avoids saddle points almost surely. Furthermore, the form of our nonlinear transformation facilitates an efficient matrix-free approach to computing the update via Krylov based quadrature, making our method scalable to high dimensional problems. </details>
- [Stochastic Lanczos Likelihood Estimation of Genomic Variance Components](https://scholar.colorado.edu/appm_gradetds/120/), Richard Border, 2018, Masters <details> Genomic variance components analysis seeks to estimate the extent to which interindividual variation in a given trait can be attributed to genetic similarity. Likelihood estimation of such models involves computationally expensive operations on large, dense, and unstructured matrices of high rank. As a result, standard estimation procedures relying on direct matrix methods become prohibitively expensive as sample sizes increase. We propose a novel estimation procedure that uses the Lanczos process and stochastic Lanczos quadrature to approximate the likelihood for an initial choice of parameter values. Then, by identifying the variance components parameter space with a family of shifted linear systems, we are able to exploit the Krylov subspace shift-invariance property to efficiently compute the likelihood for all additional parameter values of interest in linear time. Numerical experiments using simulated data demonstrate increased performance relative to conventional methods with little loss of accuracy. </details>
- [A comparison of spectral estimation techniques](https://scholar.colorado.edu/appm_gradetds/147/), Marc Thomson, 2019, Masters; code at [github](https://github.com/MarcThomson/MDSpectralAnalysis). <details> Compared with experiments, molecular dynamics (MD) simulations provide a quick and inexpensive way to study the properties of chemical systems. In many cases, it is necessary to extract spectral data from these simulations, such as infrared or Raman spectra. For instance, to validate that the computational system matches a physical system, the spectral 'fingerprints' can be examined. For complicated systems, Raman spectroscopy calculations are computationally expensive, providing an incentive to reduce the amount of data required. Currently, spectral estimation from MD simulations relies on the discrete Fourier transform (DFT); however, alternative methods can more precisely model the spectra using fewer data points. These methods are particularly effective when prior knowledge of the spectral shape is considered. Several methods, including the direct regression, Welch power estimation, the regularized resolvent transform (RRT), and a modified version of the filter diagonalization method (FDM) are compared to the DFT when applied to MD simulations of methanol and sodium chloride. We propose a novel modification of the FDM, including use of the LASSO (least absolute shrinkage and selection operator) to improve the method's accuracy. Moreover, 'windowing' present in FDM is modified to produce a significantly more accurate spectrum. The performance of these methods is then compared with each other to determine which methods are prone to include incorrect spectral features or lack correct spectral features. In brief, the modified FDM and RRT far outperformed other methods: the modified FDM produces the lowest rate incorrect spectral peaks while the RRT produces the lowest rate of missing peaks. </details>
- [Optimization for High-Dimensional Data Analysis](https://scholar.colorado.edu/appm_gradetds/86/), Derek Driggs, 2017, Masters <details>As modern datasets continue to grow in size, they are also growing in complexity. Data are more often being recorded using multiple sensors, creating large, multidimensional datasets that are difficult to analyze. In this thesis, we explore methods to accelerate low-rank recovery algorithms for data analysis, with an emphasis on Robust Principal Component Analysis (RPCA). We also develop a tensor-based approach to RPCA that preserves the inherent structure of multidimensional datasets, allowing for improved analysis. Both of our approaches use nuclear-norm regularization with Burer-Monteiro factorization (or higher-order generalizations thereof) to transform convex but expensive programs into non-convex programs that can be solved efficiently. We supplement our non-convex programs with a certificate of optimality that can be used to bound the suboptimality of each iterate. We demonstrate that both of these approaches allow for new applications of RPCA in fields involving multidimensional datasets; for example, we show that our methods can be used for real-time video processing as well as the analysis of fMRI brain-scans. Traditionally, these tasks have been considered too demanding for low-rank recovery algorithms.</details>

Undergraduate theses
- [Improving Existing Bayesian Optimization Software by Incorporating Gradient Information](https://github.com/yyexela/botorch/blob/main/tutorials/fobo.ipynb), Alexey Yermakov (CS and APPM), CS Capstone senior thesis, 2023 <details>This tutorial demonstrates how to use gradient information in Bayesian Optimization. Traditionally, Bayesian Optimization is a zeroth-order method that does not utilize gradient information because oftentimes the black-box functions being optimized does not provide gradients. However, there may be some cases where gradient information is available [1]. As a result, derivative-enabled Bayesian Optimization may use the additional gradient information. We will call zeroth-order Bayesian Optimization ZOBO and first-order Bayesian Optimization FOBO. </details>
- [A Generalization of S-Divergence to Symmetric Cone Programming via Euclidean Jordan Algebra (local copy)](/assets/docs/Jaden_Wang_2022_thesis.pdf), Zhuochen (Jaden) Wang, 2022, BS <details> Symmetric cone programming encompasses a vast majority of tractable convex optimization problems, including linear programming, semidefinite programming, and second-order cone programming. It turns out that we can generalize many results from semidefinite matrices to symmetric cones under the abstract framework of Euclidean Jordan algebra. In particular, S-divergence was previously proposed as a numerical alternative to Riemannian distance for the Hermitian positive definite cone. The goal of this thesis is to generalize S-divergence to symmetric cones and prove that its nice properties in the matrix case are preserved. Specifically, we wish to show that S-divergence induces a metric on the cone and is geodesically-convex. After an extensive exposition of necessary background, we successfully proved most of our claims, with only one conjecture remaining to be proven. </details>

Professional Masters "culminating experience"
- Austin Wagenknecht, 2022, Digital Signal Processing Methods in Music Information Retrieval
- Jacob Tiede, 2021, [Getting Started in Computational Genetics in Python](https://github.com/Jacob-Tie/GraduateSchoolCourseWork/blob/master/Capstone_Project/Article.md)

## Miscellany

- [Open Problem: Property Elicitation and Elicitation Complexity](http://www.jmlr.org/proceedings/papers/v49/frongillo16.html), Rafael Frongillo, Ian Kash, Stephen Becker. 29th Annual Conference on Learning Theory (COLT), pp. 1655–1658, 2016
- [Trig identities](/assets/docs/trig.pdf) that I used in high school and college

